{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":52254,"databundleVersionId":6863140,"sourceType":"competition"},{"sourceId":2021296,"sourceType":"datasetVersion","datasetId":1121486},{"sourceId":3170856,"sourceType":"datasetVersion","datasetId":1927562},{"sourceId":3899138,"sourceType":"datasetVersion","datasetId":2316152},{"sourceId":4191832,"sourceType":"datasetVersion","datasetId":2472169},{"sourceId":4204135,"sourceType":"datasetVersion","datasetId":2478634},{"sourceId":4311652,"sourceType":"datasetVersion","datasetId":2539716},{"sourceId":4417241,"sourceType":"datasetVersion","datasetId":2587776},{"sourceId":4421029,"sourceType":"datasetVersion","datasetId":2589631},{"sourceId":6671075,"sourceType":"datasetVersion","datasetId":3849078},{"sourceId":148241080,"sourceType":"kernelVersion"},{"sourceId":157039748,"sourceType":"kernelVersion"},{"sourceId":157156630,"sourceType":"kernelVersion"}],"dockerImageVersionId":30236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DEBUG = False\nINPUT = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\nSTAGE_2_MODELS = '/kaggle/input/rsna23-tsl-lrh-efb5'\nBACKBONE_2 = 'tf_efficientnet_b5_ap'","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:16:27.690708Z","iopub.execute_input":"2024-01-01T07:16:27.691218Z","iopub.status.idle":"2024-01-01T07:16:27.69768Z","shell.execute_reply.started":"2024-01-01T07:16:27.691177Z","shell.execute_reply":"2024-01-01T07:16:27.696623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nsys.path = [\n    '../input/covn3d-same',\n    '../input/timm20221011/pytorch-image-models-master',\n    '../input/smp20210127/segmentation_models.pytorch-master/segmentation_models.pytorch-master',\n    '../input/smp20210127/pretrained-models.pytorch-master/pretrained-models.pytorch-master',\n    '../input/smp20210127/EfficientNet-PyTorch-master/EfficientNet-PyTorch-master',\n] + sys.path\n\n!pip -q install ../input/pylibjpeg140py3/pylibjpeg-1.4.0-py3-none-any.whl\n!pip -q install ../input/pylibjpeg140py3/python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n!cp -r ../input/timm-20220211/pytorch-image-models-master/timm ./timm4smp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T07:16:27.722097Z","iopub.execute_input":"2024-01-01T07:16:27.722579Z","iopub.status.idle":"2024-01-01T07:17:41.496958Z","shell.execute_reply.started":"2024-01-01T07:16:27.722537Z","shell.execute_reply":"2024-01-01T07:17:41.495387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom collections import Counter\nimport psutil\nimport shutil\nimport ctypes\nfrom fastcore.all import Path\n\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfrom collections import defaultdict\nimport ast\nimport cv2\nimport time\nimport timm\nimport timm4smp\nimport pickle\nimport random\nimport pydicom\nimport argparse\nimport warnings\nimport threading\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom glob import glob\nimport albumentations\nimport matplotlib.pyplot as plt\nimport segmentation_models_pytorch as smp\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom pylab import rcParams\n\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntorch.backends.cudnn.benchmark = True\n\ntimm.__version__, timm4smp.__version__","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:41.501007Z","iopub.execute_input":"2024-01-01T07:17:41.501583Z","iopub.status.idle":"2024-01-01T07:17:51.514892Z","shell.execute_reply.started":"2024-01-01T07:17:41.501522Z","shell.execute_reply":"2024-01-01T07:17:51.513566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ram_usage(threshold: float=None): \n    \"Returns ram usage in GB, garbage collecting if its over `threshold`.\"\n    process = psutil.Process(os.getpid())\n    memory_usage_bytes = process.memory_info().rss\n    ram_usage = memory_usage_bytes / (1000 ** 3)\n    if threshold and ram_usage > threshold:\n        print(f'ram usage: {ram_usage}, garbage collecting')\n        libc = ctypes.CDLL(\"libc.so.6\")\n        libc.malloc_trim(0)\n        gc.collect()\n        print(f'new ram usage: {process.memory_info().rss / (1000 ** 3)}')\n    return ram_usage","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.516741Z","iopub.execute_input":"2024-01-01T07:17:51.517457Z","iopub.status.idle":"2024-01-01T07:17:51.52888Z","shell.execute_reply.started":"2024-01-01T07:17:51.517415Z","shell.execute_reply":"2024-01-01T07:17:51.527112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru = get_ram_usage","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.53199Z","iopub.execute_input":"2024-01-01T07:17:51.532572Z","iopub.status.idle":"2024-01-01T07:17:51.556784Z","shell.execute_reply.started":"2024-01-01T07:17:51.532516Z","shell.execute_reply":"2024-01-01T07:17:51.555292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigating studies per patient","metadata":{}},{"cell_type":"code","source":"if DEBUG: \n    met = pd.read_parquet('/kaggle/input/rsna-2023-abdominal-trauma-detection/train_dicom_tags.parquet')\n    met = met.rename(columns={'PatientID': 'patient_id'})\n\n    x =  met.SeriesInstanceUID.nunique()/ met.patient_id.nunique()\n    print(x, 'scans per patient')\n    print(f'therefore, there should be around {1100 * x} scans in the test set')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.558251Z","iopub.execute_input":"2024-01-01T07:17:51.558634Z","iopub.status.idle":"2024-01-01T07:17:51.571595Z","shell.execute_reply.started":"2024-01-01T07:17:51.558601Z","shell.execute_reply":"2024-01-01T07:17:51.570153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir_seg = '/kaggle/input/rsna23-train-stage1/models'\nimage_size_seg = (128, 128, 128)\nimage_size = 224\nmsk_size = image_size_seg[0]\nimage_size_cls = 224\nn_slice_per_c = 15\nn_ch = 5\n\nbatch_size_seg = 1\nnum_workers = 4","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.573399Z","iopub.execute_input":"2024-01-01T07:17:51.574158Z","iopub.status.idle":"2024-01-01T07:17:51.587136Z","shell.execute_reply.started":"2024-01-01T07:17:51.574117Z","shell.execute_reply":"2024-01-01T07:17:51.585877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make dataframe of studies with dicoms","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/rsna23-local-test-set/df_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.588881Z","iopub.execute_input":"2024-01-01T07:17:51.589591Z","iopub.status.idle":"2024-01-01T07:17:51.62977Z","shell.execute_reply.started":"2024-01-01T07:17:51.589545Z","shell.execute_reply":"2024-01-01T07:17:51.628645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n    \"\"\"\n    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n    \"\"\"\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\n    pixel_array = dcm.pixel_array\n    if dcm.PixelRepresentation == 1:\n        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n        dtype = pixel_array.dtype \n        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n#         pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n\n    intercept = float(dcm.RescaleIntercept)\n    slope = float(dcm.RescaleSlope)\n    center = int(dcm.WindowCenter)\n    width = int(dcm.WindowWidth)\n    low = center - width / 2\n    high = center + width / 2    \n    \n    pixel_array = (pixel_array * slope) + intercept\n    pixel_array = np.clip(pixel_array, low, high)\n\n    return pixel_array\n\n\ndef study_path_to_3D_image(path, image_size_seg=(128, 128, 128), plot_image=False, z_df=None):\n    t_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n    t_paths = [p for p in t_paths if '3124/5842/514.dcm' not in p] # corrupted file\n    n_scans = len(t_paths)\n    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_size_seg[2])).round().astype(int)\n    t_paths = [t_paths[i] for i in indices]\n\n    imgs = {}\n    pos_zs = []\n    for filename in t_paths:\n        dicom = pydicom.dcmread(filename)\n        pos_z = dicom[(0x20, 0x32)].value[-1]  # to retrieve the order of frames\n#         dicom_numbers.append(filename.split('/')[-1].split(\".\")[0])\n        pos_zs.append(pos_z)\n        img = standardize_pixel_array(dicom)\n        img = cv2.resize(img, (image_size_seg[0], image_size_seg[1]), interpolation = cv2.INTER_AREA)\n        if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n            img = 1 - img\n        imgs[pos_z] = img\n        \n#     print(dicom_numbers, pos_zs)\n    if len(pos_zs) > 1: \n        pos_z_ascending = pos_zs[-1] > pos_zs[0]\n    else: \n        pos_z_ascending = True\n    if z_df is not None: \n        study = path.split('/')[-1]\n        z_df.loc[study, 'pos_z_ascending'] = pos_zs[-1] > pos_zs[0]\n        \n    images = []\n    cnt = Counter(pos_zs)\n    for i, k in enumerate(sorted(imgs.keys())):\n        img = imgs[k]\n#         images.append(img)\n        images.extend([img] * cnt[k]) # to make sure we have same dimensions\n        if not (i % 100) and plot_image:\n            plt.figure(figsize=(5, 5))\n            plt.imshow(img, cmap=\"gray\")\n            plt.title(f\"Patient {patient} - Study {study} - Frame {i}/{len(imgs)}\")\n            plt.axis(False)\n            plt.show()\n    images = np.stack(images, -1)\n    \n    images = images - np.min(images)\n    images = images / (np.max(images) + 1e-4)\n    images = (images * 255).astype(np.uint8)\n    return images","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.631475Z","iopub.execute_input":"2024-01-01T07:17:51.63213Z","iopub.status.idle":"2024-01-01T07:17:51.65943Z","shell.execute_reply.started":"2024-01-01T07:17:51.63209Z","shell.execute_reply":"2024-01-01T07:17:51.657832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ORGANS = ['liver']\nLABELS = [['liver_healthy', 'liver_low', 'liver_high']]\nABS = [[0, 15]]\nN_ORGANS = len(ORGANS)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.661758Z","iopub.execute_input":"2024-01-01T07:17:51.66229Z","iopub.status.idle":"2024-01-01T07:17:51.684205Z","shell.execute_reply.started":"2024-01-01T07:17:51.66214Z","shell.execute_reply":"2024-01-01T07:17:51.682776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_sample(row, has_mask=True):\n    image = study_path_to_3D_image(row.image_folder)\n    if image.ndim < 4:\n        image = np.expand_dims(image, 0) # to 3ch\n    return image.repeat(3, 0) \n\n\nclass SegTestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df = df.reset_index()\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n\n        image = load_sample(row, has_mask=False)\n        image = image / 255.\n#         gc.collect()\n        return torch.tensor(image).float()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.690526Z","iopub.execute_input":"2024-01-01T07:17:51.690947Z","iopub.status.idle":"2024-01-01T07:17:51.704752Z","shell.execute_reply.started":"2024-01-01T07:17:51.690911Z","shell.execute_reply":"2024-01-01T07:17:51.703324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_seg = SegTestDataset(df)\n# display(df.head())\nloader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.706278Z","iopub.execute_input":"2024-01-01T07:17:51.706804Z","iopub.status.idle":"2024-01-01T07:17:51.726447Z","shell.execute_reply.started":"2024-01-01T07:17:51.706752Z","shell.execute_reply":"2024-01-01T07:17:51.724974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Getting the raw images and the processed images which are ready \\n\\\nto input into the segmentation model')\nidx = 0\nprocessed_images = dataset_seg[idx]\npath = df.iloc[idx].image_folder\nt_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\nraw_imgs = [pydicom.dcmread(f).pixel_array for f in t_paths]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:51.728105Z","iopub.execute_input":"2024-01-01T07:17:51.728673Z","iopub.status.idle":"2024-01-01T07:17:57.011843Z","shell.execute_reply.started":"2024-01-01T07:17:51.72862Z","shell.execute_reply":"2024-01-01T07:17:57.01043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original animation code from https://www.kaggle.com/code/franklinshih0617/rsna-abdominal-trauma-detect-eda-animation\ndef animate_images(images, figsize=(6, 6), cmap=None):\n    \"Run HTML({returned_object}) to see animation\"\n    fig, ax = plt.subplots(figsize=figsize)\n    im = ax.imshow(images[0], cmap=cmap) \n    def update(i): \n        im.set_array(images[i])\n    ani = animation.FuncAnimation(fig, update, frames=range(len(images)), repeat=True)\n    return ani.to_jshtml()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:57.013923Z","iopub.execute_input":"2024-01-01T07:17:57.014322Z","iopub.status.idle":"2024-01-01T07:17:57.022939Z","shell.execute_reply.started":"2024-01-01T07:17:57.014285Z","shell.execute_reply":"2024-01-01T07:17:57.021571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(animate_images(raw_imgs[::2], cmap='gray'))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:17:57.024787Z","iopub.execute_input":"2024-01-01T07:17:57.025651Z","iopub.status.idle":"2024-01-01T07:18:04.304055Z","shell.execute_reply.started":"2024-01-01T07:17:57.025599Z","shell.execute_reply":"2024-01-01T07:18:04.302389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = (processed_images[0].permute(2, 0, 1).numpy() * 255).astype(int)\nprint(x.min(), x.max())\nx = list(x)\nHTML(animate_images(x, cmap='gray'))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:04.305897Z","iopub.execute_input":"2024-01-01T07:18:04.30628Z","iopub.status.idle":"2024-01-01T07:18:17.451363Z","shell.execute_reply.started":"2024-01-01T07:18:04.306245Z","shell.execute_reply":"2024-01-01T07:18:17.449496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### See the raw input data: a slideshow of the ct scan\n   ","metadata":{}},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"drop_rate = 0.\ndrop_path_rate = 0.\nout_dim = 5\nclass TimmSegModel(nn.Module):\n    def __init__(self, backbone, segtype='unet', pretrained=False):\n        super(TimmSegModel, self).__init__()\n\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=3,\n            features_only=True,\n            drop_rate=drop_rate,\n            drop_path_rate=drop_path_rate,\n            pretrained=pretrained\n        )\n        g = self.encoder(torch.rand(1, 3, 64, 64))\n        encoder_channels = [1] + [_.shape[1] for _ in g]\n        decoder_channels = [256, 128, 64, 32, 16]\n        if segtype == 'unet':\n            self.decoder = smp.unet.decoder.UnetDecoder(\n                encoder_channels=encoder_channels[:n_blocks+1],\n                decoder_channels=decoder_channels[:n_blocks],\n                n_blocks=n_blocks,\n            )\n\n        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n    def forward(self,x):\n        global_features = [0] + self.encoder(x)[:n_blocks]\n        seg_features = self.decoder(*global_features)\n        seg_features = self.segmentation_head(seg_features)\n        return seg_features\n\nfrom timm.models.layers.conv2d_same import Conv2dSame\nfrom conv3d_same import Conv3dSame\n\n\ndef convert_3d(module):\n\n    module_output = module\n    if isinstance(module, torch.nn.BatchNorm2d):\n        module_output = torch.nn.BatchNorm3d(\n            module.num_features,\n            module.eps,\n            module.momentum,\n            module.affine,\n            module.track_running_stats,\n        )\n        if module.affine:\n            with torch.no_grad():\n                module_output.weight = module.weight\n                module_output.bias = module.bias\n        module_output.running_mean = module.running_mean\n        module_output.running_var = module.running_var\n        module_output.num_batches_tracked = module.num_batches_tracked\n        if hasattr(module, \"qconfig\"):\n            module_output.qconfig = module.qconfig\n            \n    elif isinstance(module, Conv2dSame):\n        module_output = Conv3dSame(\n            in_channels=module.in_channels,\n            out_channels=module.out_channels,\n            kernel_size=module.kernel_size[0],\n            stride=module.stride[0],\n            padding=module.padding[0],\n            dilation=module.dilation[0],\n            groups=module.groups,\n            bias=module.bias is not None,\n        )\n        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n\n    elif isinstance(module, torch.nn.Conv2d):\n        module_output = torch.nn.Conv3d(\n            in_channels=module.in_channels,\n            out_channels=module.out_channels,\n            kernel_size=module.kernel_size[0],\n            stride=module.stride[0],\n            padding=module.padding[0],\n            dilation=module.dilation[0],\n            groups=module.groups,\n            bias=module.bias is not None,\n            padding_mode=module.padding_mode\n        )\n        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n\n    elif isinstance(module, torch.nn.MaxPool2d):\n        module_output = torch.nn.MaxPool3d(\n            kernel_size=module.kernel_size,\n            stride=module.stride,\n            padding=module.padding,\n            dilation=module.dilation,\n            ceil_mode=module.ceil_mode,\n        )\n    elif isinstance(module, torch.nn.AvgPool2d):\n        module_output = torch.nn.AvgPool3d(\n            kernel_size=module.kernel_size,\n            stride=module.stride,\n            padding=module.padding,\n            ceil_mode=module.ceil_mode,\n        )\n\n    for name, child in module.named_children():\n        module_output.add_module(\n            name, convert_3d(child)\n        )\n    del module\n\n    return module_output\n\n\n# backbone = 'resnet18d'\n# n_blocks = 4\n# model = TimmSegModel(backbone)\n# model = convert_3d(model)\n# model(torch.rand(1, 3, 128,128,128)).shape\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:17.453498Z","iopub.execute_input":"2024-01-01T07:18:17.453905Z","iopub.status.idle":"2024-01-01T07:18:17.49526Z","shell.execute_reply.started":"2024-01-01T07:18:17.453867Z","shell.execute_reply":"2024-01-01T07:18:17.49377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \nclass TimmModel(nn.Module):\n    def __init__(self, backbone, pretrained=False, out_dim=3, h=image_size, w=image_size):\n        super(TimmModel, self).__init__()\n        self.h = h\n        self.w = w\n\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=in_chans,\n            num_classes=out_dim,\n            features_only=False,\n            drop_rate=drop_rate,\n            drop_path_rate=drop_path_rate,\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(drop_rate_last),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, out_dim), # chacnged\n        )\n\n    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n        bs = x.shape[0]\n        x = x.view(bs * n_slice_per_c, in_chans, self.h, self.w)\n        feat = self.encoder(x)\n        feat = feat.view(bs, n_slice_per_c, -1)\n        feat, _ = self.lstm(feat)\n        feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n        feat = self.head(feat)\n        feat = feat.view(bs, n_slice_per_c, -1).contiguous()\n\n        return feat","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:17.497146Z","iopub.execute_input":"2024-01-01T07:18:17.497616Z","iopub.status.idle":"2024-01-01T07:18:17.514836Z","shell.execute_reply.started":"2024-01-01T07:18:17.497574Z","shell.execute_reply":"2024-01-01T07:18:17.513498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Models","metadata":{}},{"cell_type":"code","source":"models_seg = []\n\nkernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\nbackbone = 'resnet18d'\nn_blocks = 4\nfor fold in range(1):\n    model = TimmSegModel(backbone, pretrained=False)\n    model = convert_3d(model)\n    load_model_file = '/kaggle/input/rsna23-train-stage1/models/timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep_fold0_best.pth'\n    sd = torch.load(load_model_file, map_location=torch.device('cpu'))\n    if 'model_state_dict' in sd.keys():\n        sd = sd['model_state_dict']\n    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n    model.load_state_dict(sd, strict=True)\n    model = model.to(device)\n    model.eval()\n    models_seg.append(model)\nlen(models_seg)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:17.517007Z","iopub.execute_input":"2024-01-01T07:18:17.517435Z","iopub.status.idle":"2024-01-01T07:18:19.84155Z","shell.execute_reply.started":"2024-01-01T07:18:17.517352Z","shell.execute_reply":"2024-01-01T07:18:19.840318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_type = '0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep'\nbackbone = BACKBONE_2\nmodel_dir_cls = F'{STAGE_2_MODELS}/models'\nin_chans = 6\ndrop_rate_last = 0.3\nmodels = [TimmModel(backbone, pretrained=False), \n#              TimmModel(backbone, pretrained=False), \n#              TimmModel(backbone, pretrained=False, h=image_size*2), \n#              TimmModel(backbone, pretrained=False, out_dim=2), \n         ]\n\nfor model, organ in zip(models, ORGANS):\n    print(organ)\n    load_model_file = f'{STAGE_2_MODELS}/models/{organ}_0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep_fold0_last.pth'\n    sd = torch.load(load_model_file, map_location=torch.device('cpu'))\n    if 'model_state_dict' in sd.keys():\n        sd = sd['model_state_dict']\n    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n    model.load_state_dict(sd, strict=True)\n    model = model.to(device)\n    model.eval()\nlen(models)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:19.843657Z","iopub.execute_input":"2024-01-01T07:18:19.844108Z","iopub.status.idle":"2024-01-01T07:18:24.85265Z","shell.execute_reply.started":"2024-01-01T07:18:19.844063Z","shell.execute_reply":"2024-01-01T07:18:24.851326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# timm.list_models()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:24.85424Z","iopub.execute_input":"2024-01-01T07:18:24.854634Z","iopub.status.idle":"2024-01-01T07:18:24.860319Z","shell.execute_reply.started":"2024-01-01T07:18:24.854599Z","shell.execute_reply":"2024-01-01T07:18:24.858575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:24.862093Z","iopub.execute_input":"2024-01-01T07:18:24.862534Z","iopub.status.idle":"2024-01-01T07:18:26.026809Z","shell.execute_reply.started":"2024-01-01T07:18:24.862485Z","shell.execute_reply":"2024-01-01T07:18:26.024362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_bone(msk, cid, t_paths, cropped_images):\n    n_scans = len(t_paths)\n    bone = []\n    try:\n        msk_b = msk[cid] > 0.2\n        msk_c = msk[cid] > 0.05\n\n        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n\n        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n\n        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n\n        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)\n        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n\n            msk_this = msk[cid, :, :, ind_]\n\n            images = []\n            for i in range(-n_ch//2+1, n_ch//2+1):\n                try:\n                    dicom = pydicom.read_file(t_paths[ind+i])\n                    img = standardize_pixel_array(dicom)\n                    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n                        img = 1 - img\n                    images.append(img)\n                except:\n                    images.append(np.zeros((512, 512)))\n\n            data = np.stack(images, -1)\n            data = data - np.min(data)\n            data = data / (np.max(data) + 1e-4)\n            data = (data * 255).astype(np.uint8)\n            msk_this = msk_this[x1:x2, y1:y2]\n            xx1 = int(x1 / msk_size * data.shape[0])\n            xx2 = int(x2 / msk_size * data.shape[0])\n            yy1 = int(y1 / msk_size * data.shape[1])\n            yy2 = int(y2 / msk_size * data.shape[1])\n            data = data[xx1:xx2, yy1:yy2]\n            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n            msk_this = (msk_this * 255).astype(np.uint8)\n            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n\n            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)\n\n            bone.append(torch.tensor(data))\n#             gc.collect()\n\n    except:\n        for sid in range(n_slice_per_c):\n            bone.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n\n    cropped_images[cid] = torch.stack(bone, 0)\n\n\ndef load_cropped_images(msk, image_folder, n_ch=n_ch):\n\n    pos_z = []\n    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n    for filename in t_paths[:2]:\n        dicom = pydicom.dcmread(filename)\n        pos_z.append(dicom[(0x20, 0x32)].value[-1])  # to retrieve the order of frames\n    if len(pos_z) > 1: z_ascending = pos_z[1] > pos_z[0] \n    else: z_ascending = True\n    if not z_ascending: t_paths.reverse()\n    for cid in range(5):\n        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images))\n        threads[cid].start()\n    for cid in range(5):\n        threads[cid].join()\n#     gc.collect()\n\n    return torch.cat(cropped_images, 0)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:26.030226Z","iopub.execute_input":"2024-01-01T07:18:26.030863Z","iopub.status.idle":"2024-01-01T07:18:26.076322Z","shell.execute_reply.started":"2024-01-01T07:18:26.030814Z","shell.execute_reply":"2024-01-01T07:18:26.074586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"# # For exceptions: \n# train = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train.csv')\n# tar_means = train.mean()\n# n_exceptions = 0","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:26.078062Z","iopub.execute_input":"2024-01-01T07:18:26.078865Z","iopub.status.idle":"2024-01-01T07:18:26.100496Z","shell.execute_reply.started":"2024-01-01T07:18:26.078801Z","shell.execute_reply":"2024-01-01T07:18:26.098563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_seg = SegTestDataset(df)\ndisplay(df.head())\nloader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, \n                                         shuffle=False, num_workers=num_workers)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:26.102843Z","iopub.execute_input":"2024-01-01T07:18:26.103361Z","iopub.status.idle":"2024-01-01T07:18:26.158713Z","shell.execute_reply.started":"2024-01-01T07:18:26.103304Z","shell.execute_reply":"2024-01-01T07:18:26.157156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npreds = defaultdict(list)\nbar = tqdm(loader_seg)\nwith torch.no_grad():\n    for batch_id, (images) in enumerate(bar):\n        try:\n            pred_masks = []\n            images = images.to(device)\n            pred_masks = []\n            for model in models_seg:\n                pmask = model(images).sigmoid()\n                pred_masks.append(pmask)\n            pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n\n            # Build cls input\n            cls_inp = []\n            threads = [None] * 5\n            cropped_images = [None] * 5\n            for i in range(pred_masks.shape[0]):\n                row = df.iloc[batch_id*batch_size_seg+i]\n                cropped_images = load_cropped_images(pred_masks[i], row.image_folder)\n                cls_inp.append(cropped_images.permute(0, 3, 1, 2).float() / 255.)\n            cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 75, 6, 224, 224)\n\n            image_full = cls_inp[0]\n            out = defaultdict(dict)\n            for organ, cols, (a, b) in zip(ORGANS, LABELS, ABS):\n                images = []\n                for image in image_full[a: b]: \n                    images.append(image.cpu())\n                images = np.stack(images, 0)\n                if organ == 'kidney': \n                    images = np.concatenate((images[:15, :, :, :], images[15:, :, :, :]), 2)\n                out[organ]['images'] = torch.tensor(images).float().to(device)\n\n            for organ, model, label_cols in zip(ORGANS, models, LABELS): \n                try: \n                    logits = model(out[organ]['images'].unsqueeze(0)).squeeze()\n                    data = logits.sigmoid()\n                    min_first_column = torch.min(data[:, 0])\n                    max_last_two_columns = torch.max(data[:, 1:], dim=0).values\n                    result = torch.cat((min_first_column.unsqueeze(0), max_last_two_columns), dim=0)\n#                     print('***********', result.shape) ###################\n                    preds[organ].append(result.cpu())\n                except: \n                    print('XXXXXXXX')\n                    preds[organ].append(torch.tensor(tar_means[label_cols]))\n        except: \n            print('problem in loop')\n            n_exceptions += 1\n            for organ, label_cols in zip(ORGANS, LABELS):\n                preds[organ].append(torch.tensor(tar_means[label_cols]))\n        get_ram_usage(1)\n        get_ram_usage(1)\n        if batch_id % 100 == 0:\n            get_ram_usage(1)\n            time.sleep(1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:18:26.162969Z","iopub.execute_input":"2024-01-01T07:18:26.163407Z","iopub.status.idle":"2024-01-01T07:18:57.856871Z","shell.execute_reply.started":"2024-01-01T07:18:26.163353Z","shell.execute_reply":"2024-01-01T07:18:57.854353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_cols = ['pred_liver_healthy', 'pred_liver_low', 'pred_liver_high']\ndf.loc[:, pred_cols] = torch.stack(preds['liver']).numpy()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T07:30:06.821388Z","iopub.execute_input":"2024-01-01T07:30:06.821829Z","iopub.status.idle":"2024-01-01T07:30:06.827028Z","shell.execute_reply.started":"2024-01-01T07:30:06.821791Z","shell.execute_reply":"2024-01-01T07:30:06.825992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('df_with_preds.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}